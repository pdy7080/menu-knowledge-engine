"""
Performance Benchmark Script - Sprint 3 P2-2
Measure API response times and throughput
"""

import requests
import time
import statistics
from typing import Dict


API_BASE_URL = "http://localhost:8000"

# Test cases
TEST_CASES = [
    ("ê¹€ì¹˜ì°Œê°œ", "exact match"),
    ("í• ë¨¸ë‹ˆê¹€ì¹˜ì°Œê°œ", "modifier decomposition"),
    ("ìŠ¤í…Œì´í¬", "AI discovery"),
    ("ë¹„ë¹”ë°¥", "exact match"),
    ("ì–¼í°ìˆœë‘ë¶€ì°Œê°œ", "modifier"),
]


def benchmark_identify_api(iterations: int = 100) -> Dict:
    """
    /api/v1/menu/identify ì—”ë“œí¬ì¸íŠ¸ ë²¤ì¹˜ë§ˆí¬

    Args:
        iterations: ë°˜ë³µ íšŸìˆ˜

    Returns:
        {
            "avg_ms": float,
            "p50_ms": float,
            "p95_ms": float,
            "p99_ms": float,
            "max_ms": float,
            "min_ms": float
        }
    """
    response_times = []

    print(f"\nğŸ”„ Benchmarking /api/v1/menu/identify ({iterations} requests)...")

    for i in range(iterations):
        menu_name, case_type = TEST_CASES[i % len(TEST_CASES)]

        start = time.time()
        response = requests.post(
            f"{API_BASE_URL}/api/v1/menu/identify", json={"menu_name_ko": menu_name}
        )
        end = time.time()

        elapsed_ms = (end - start) * 1000
        response_times.append(elapsed_ms)

        if (i + 1) % 10 == 0:
            print(f"  Progress: {i + 1}/{iterations} requests")

    # Calculate statistics
    response_times.sort()

    results = {
        "avg_ms": statistics.mean(response_times),
        "median_ms": statistics.median(response_times),
        "p50_ms": response_times[len(response_times) // 2],
        "p95_ms": response_times[int(len(response_times) * 0.95)],
        "p99_ms": response_times[int(len(response_times) * 0.99)],
        "max_ms": max(response_times),
        "min_ms": min(response_times),
    }

    return results


def print_results(results: Dict):
    """ê²°ê³¼ ì¶œë ¥"""
    print("\n" + "=" * 50)
    print("ğŸ“Š Performance Benchmark Results")
    print("=" * 50)
    print(f"Average:    {results['avg_ms']:.2f} ms")
    print(f"Median:     {results['median_ms']:.2f} ms")
    print(f"P50:        {results['p50_ms']:.2f} ms")
    print(f"P95:        {results['p95_ms']:.2f} ms")
    print(f"P99:        {results['p99_ms']:.2f} ms")
    print(f"Max:        {results['max_ms']:.2f} ms")
    print(f"Min:        {results['min_ms']:.2f} ms")
    print("=" * 50)

    # Performance assessment
    p95 = results["p95_ms"]
    if p95 < 100:
        print("âœ… Excellent performance (P95 < 100ms)")
    elif p95 < 500:
        print("âœ… Good performance (P95 < 500ms)")
    elif p95 < 1000:
        print("âš ï¸  Acceptable performance (P95 < 1s)")
    elif p95 < 3000:
        print("âš ï¸  Needs optimization (P95 < 3s)")
    else:
        print("âŒ Poor performance (P95 >= 3s)")

    print("\nTarget: P95 < 2000ms (2 seconds)")
    print(f"Current: P95 = {p95:.0f}ms")

    if p95 < 2000:
        print("ğŸ¯ TARGET ACHIEVED! âœ…")
    else:
        print(f"âŒ Need {p95 - 2000:.0f}ms improvement")


def run_benchmark():
    """Run full benchmark suite"""
    print("ğŸš€ Starting Performance Benchmark...")
    print(f"API URL: {API_BASE_URL}")

    # Health check
    try:
        response = requests.get(f"{API_BASE_URL}/health")
        if response.status_code == 200:
            print("âœ… API server is running")
        else:
            print("âŒ API server health check failed")
            return
    except Exception as e:
        print(f"âŒ Cannot connect to API server: {e}")
        return

    # Run benchmark
    results = benchmark_identify_api(iterations=100)
    print_results(results)


if __name__ == "__main__":
    run_benchmark()
